# Fichier: clipai_runpod_engine/engine/whisper_gpu.py
import os
import platform
from faster_whisper import WhisperModel

SYSTEM = platform.system().lower()
WHISPER_MODEL = None

try:
    if SYSTEM == "darwin":
        WHISPER_MODEL = WhisperModel("small", device="cpu", compute_type="int8")
        print("‚ö†Ô∏è Mod√®le 'small' charg√© pour CPU (D√©veloppement Local)")
    else:
        # Cas LINUX + CUDA (RunPod Serverless)
        # En passant juste le nom "medium", faster-whisper cherche dans le cache par d√©faut (/root/.cache).
        WHISPER_MODEL = WhisperModel("medium", device="cuda", compute_type="float16")
        print("‚ö° Mod√®le 'medium' charg√© pour Whisper GPU (Production)")

except Exception as e:
    print(f"FATAL ERROR: √âchec du chargement du mod√®le Whisper : {e}")
    # Cette erreur est ce que vous voyez dans les logs.
    raise RuntimeError("Impossible d'initialiser le mod√®le Whisper GPU.")


def transcribe_gpu(video_path):
    # Utilise le mod√®le global WHISPER_MODEL d√©j√† charg√©.
    if WHISPER_MODEL is None:
        raise RuntimeError("Le mod√®le Whisper n'a pas pu √™tre charg√© au d√©marrage du Worker.")

    print("üéß D√©marrage de la transcription...")
    segments, _ = WHISPER_MODEL.transcribe(video_path) 
    
    results = []
    for seg in segments:
        results.append({
            "start": seg.start,
            "end": seg.end,
            "text": seg.text.strip()
        })
    return results