# Fichier: clipai_runpod_engine/engine/whisper_gpu.py

import os
import platform

from faster_whisper import WhisperModel

# ==========================================================
# üöÄ AJOUT D'UNE INITIALISATION GLOBALE
# ==========================================================
# Ceci charge le mod√®le UNE FOIS au d√©marrage du Worker (RUN).
# Le mod√®le a √©t√© t√©l√©charg√© pendant le BUILD (Dockerfile).
SYSTEM = platform.system().lower()

if SYSTEM == "darwin":
    # Cas Mac (pour le d√©veloppement local)
    WHISPER_MODEL = WhisperModel("small", device="cpu", compute_type="int8")
    print("‚ö†Ô∏è Mod√®le 'small' charg√© pour CPU (D√©veloppement Local)")
else:
    # Cas LINUX + CUDA (RunPod Serverless)
    # Le mod√®le 'medium' est d√©j√† sur le disque gr√¢ce au Dockerfile
    WHISPER_MODEL = WhisperModel("medium", device="cuda", compute_type="float16")
    print("‚ö° Mod√®le 'medium' charg√© pour Whisper GPU (Production)")
# ==========================================================


def transcribe_gpu(video_path):
    """
    Fonction de transcription. Utilise le mod√®le global WHISPER_MODEL.
    """
    
    # La logique de d√©tection de plateforme est d√©sormais inutile ici, car
    # le mod√®le est initialis√© une seule fois de mani√®re globale
    
    segments, _ = WHISPER_MODEL.transcribe(video_path) 
    
    results = []
    for seg in segments:
        results.append({
            "start": seg.start,
            "end": seg.end,
            "text": seg.text.strip()
        })

    return results

# NOTE: La fonction de pr√©-t√©l√©chargement n'est plus n√©cessaire dans le Worker
# car le mod√®le est initialis√© de mani√®re globale.